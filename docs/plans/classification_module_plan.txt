================================================================================
PLAN: Consolidate difference_detection → classification + misclassification
================================================================================
Date: 2026-02-21
Branch: claude/add-misclassification-results-JVNdh
Author: claude


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
PART 0 — CONTEXT
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Current state
  src/analyze/difference_detection/          ← everything classification lives here
    __init__.py
    classification_test.py                   ← binary OvR entry point
    classification_test_multiclass.py        ← multiclass + run_classification_test()
    classification_test_viz.py               ← AUROC curves, feature comparison grid
    results.py                               ← MulticlassOVRResults, ComparisonSpec
    permutation_utils.py                     ← legacy permutation helpers
    distribution_test.py
    distance_metrics.py
    penetrance_threshold.py
    horizon_plots/
    tests/

New framework (just pushed to main, already checked out):
  src/analyze/utils/resampling/             ← unified bootstrap/permutation engine
    __init__.py                              ← resample.bootstrap(), .permute_labels(),
    _engine.py                                 .permute_groups(), .run(), .aggregate()
    _spec.py, _statistic.py, ...
    README.md                               ← migration map for legacy code

Key migration mapping from resampling/README.md:
  PR #4 | _permutation_test_ovr()           → permute_labels(within="time_strata")
         | classification_test_multiclass.py   (HIGHEST PRIORITY — used everywhere)

Goal: rename the module, wire in the new resampling framework, add misclassification.


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
PART 1 — MODULE RENAME
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Action
  Rename directory:
    src/analyze/difference_detection/  →  src/analyze/classification/

  All filenames stay identical. Contents are unchanged in this step.

Import updates (grep + sed pass):
  "from analyze.difference_detection"       → "from analyze.classification"
  "from .difference_detection"              → "from .classification"
  "import analyze.difference_detection"     → "import analyze.classification"

Files likely affected (search before touching):
  - results/mcolon/*/scripts/*.py
  - src/analyze/__init__.py (if it re-exports difference_detection)
  - Any notebook that imports from difference_detection

Backward-compat shim (optional, remove after 1 sprint):
  src/analyze/difference_detection/__init__.py  (keep as stub)
    from analyze.classification import *   # noqa: F401, F403
    import warnings
    warnings.warn(
        "analyze.difference_detection is renamed to analyze.classification",
        DeprecationWarning, stacklevel=2
    )


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
PART 2 — PERMUTATION BACKEND MIGRATION (permutation_utils → resampling)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Target file: src/analyze/classification/classification_test_multiclass.py
Target function: _permutation_test_ovr()

Current implementation (simplified):
  for perm in range(n_permutations):
      y_shuffled = label_shuffle(y, time_strata)
      score = cross_val_auroc(X, y_shuffled)
      null_dist.append(score)
  pval = compute_pvalue(observed, null_dist)

New implementation using resampling framework:
  from analyze.utils.resampling import resample

  def _auroc_scorer(data, rng):
      y_perm = data["labels"]   # already shuffled by engine
      return cross_val_auroc(data["X"], y_perm, ...)

  spec = resample.permute_labels(within="time_strata")
  stat = resample.statistic(
      "auroc", _auroc_scorer,
      default_alternative="greater"
  )
  out = resample.run(
      data={"labels": y, "X": X, "time_strata": time_strata},
      spec=spec, stat=stat,
      n_iters=n_permutations, seed=random_state, n_jobs=n_jobs,
  )
  summary = resample.aggregate(out)
  pval = summary.pvalue
  null_dist = np.array(out.samples)

Benefits:
  - SeedSequence-based RNG → reproducible + fork-safe
  - n_jobs parallelism already handled by engine
  - PermutationSummary compatible with downstream result containers

Keep permutation_utils.py for now (backward compat). Add deprecation warning
on compute_pvalue() and label_shuffle(). Remove in a follow-up PR.


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
PART 3 — pred_table AUGMENTATION (minimal surgery on existing file)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

File: src/analyze/classification/classification_test_multiclass.py
Location: ~line 719, inside per-embryo prediction loop where `probs` is in scope

Two columns to add to the per-embryo prediction record:

  # p_true: model confidence in the TRUE label (low → model is uncertain/wrong)
  true_col_idx = label_encoder.transform([true_label])[0]
  pred_record["p_true"] = float(probs[j, true_col_idx])

  # p_pred: model confidence in its own prediction (max probability)
  pred_record["p_pred"] = float(probs[j].max())

These are derivable from pred_proba_* columns but are pre-computed for
convenience. The augmented embryo_predictions df then has:
  embryo_id | time_bin | time_bin_center | true_label | pred_label |
  is_correct | pred_proba_{CE} | pred_proba_{WT} | ... | p_true | p_pred


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
PART 4 — NEW FILES IN src/analyze/classification/
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Final module layout after all changes:

  src/analyze/classification/
  ├── __init__.py
  ├── classification_test.py              (unchanged, binary OvR)
  ├── classification_test_multiclass.py   (Part 2+3 changes)
  ├── classification_test_viz.py          (unchanged)
  ├── results.py                          (unchanged, MulticlassOVRResults)
  ├── permutation_utils.py                (kept, deprecation warnings added)
  ├── distribution_test.py                (unchanged)
  ├── distance_metrics.py                 (unchanged)
  ├── penetrance_threshold.py             (unchanged)
  ├── misclassification_metrics.py        ← NEW (Part 4a)
  ├── misclassification_null.py           ← NEW (Part 4b)
  ├── misclassification_flagging.py       ← NEW (Part 4c)
  ├── misclassification_viz.py            ← NEW (Part 4d)
  ├── horizon_plots/                      (unchanged)
  └── tests/
      ├── (existing tests)
      ├── test_misclassification_metrics.py  ← NEW
      └── test_misclassification_null.py     ← NEW


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
PART 4a — misclassification_metrics.py
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Input: embryo_predictions DataFrame from run_classification_test()
  columns: embryo_id, time_bin, true_label, pred_label,
           is_correct, p_true, p_pred, pred_proba_*

def compute_per_embryo_metrics(
    embryo_predictions: pd.DataFrame,
    embryo_id_col: str = "embryo_id",
    t_bin_col: str = "time_bin",
    pred_col: str = "pred_label",
    true_col: str = "true_label",
    is_correct_col: str = "is_correct",
    p_true_col: str = "p_true",
    p_pred_col: str = "p_pred",
) -> pd.DataFrame:
    """
    Returns one row per embryo with these columns:
      embryo_id       — identifier
      true_class      — true genotype/group
      n_windows       — number of time bins observed
      n_wrong         — number of bins where pred != true
      wrong_rate      — n_wrong / n_windows
      longest_wrong_streak    — max consecutive wrong bins
      longest_correct_streak  — max consecutive correct bins
      flip_rate       — fraction of adjacent-bin transitions where pred_label changes
      mean_p_true     — mean model confidence in true class across bins
      min_p_true      — min (worst-case confidence in true class)
      mean_margin     — mean (p_true - p_pred) when wrong  [negative = confident wrong]
      top_confused_class      — most frequent predicted class when wrong
      top_confused_frac       — fraction of wrong bins pointing to top_confused_class
      n_class_switches        — number of distinct pred_labels seen when wrong
    """


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
PART 4b — misclassification_null.py
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Uses: analyze.utils.resampling

def null_test_wrong_rate(
    per_embryo_metrics: pd.DataFrame,
    embryo_predictions: pd.DataFrame,
    n_permutations: int = 1_000,
    stratify_by: str = "time_bin",
    random_state: int = 42,
    n_jobs: int = 1,
) -> pd.DataFrame:
    """
    For each embryo: is its wrong_rate significantly elevated vs a
    label-permutation null of the SAME size/time distribution?

    Strategy:
      - Compute global error rate per true_class and time_bin (expected baseline)
      - Use resample.permute_labels(within="time_bin") to shuffle pred_label
        within each time bin (preserves class-time marginals)
      - Statistic = wrong_rate over the embryo's observed time bins

    Returns per_embryo_metrics with added columns:
      wrong_rate_null_mean, wrong_rate_null_std
      pval_wrong_rate         (permutation, one-tailed: observed > null)
      qval_wrong_rate         (BH FDR correction across all embryos)
    """

    # Pattern for each embryo:
    def _wrong_rate_fn(data, rng):
        labels = data["labels"]     # shuffled by engine
        return (labels != data["true_vec"]).mean()

    spec = resample.permute_labels(within="time_bin")
    stat = resample.statistic("wrong_rate", _wrong_rate_fn, is_nonnegative=True)
    # ... loop over embryos, collect PermutationSummary per embryo


def null_test_streak(
    per_embryo_metrics: pd.DataFrame,
    embryo_predictions: pd.DataFrame,
    n_sim: int = 10_000,
    random_state: int = 42,
) -> pd.DataFrame:
    """
    Bernoulli model for longest wrong streak.

    For embryo i with n_windows bins and estimated error rate e_hat (from
    class-time stratified baseline), simulate Bernoulli(e_hat) sequences of
    length n_windows and record the distribution of longest-run lengths.

    Uses resample.bootstrap(size=n_windows) applied to a Bernoulli draw,
    which cleanly maps onto the engine.

    Returns:
      pval_streak   (fraction of sims with streak >= observed)
      qval_streak   (BH FDR)
    """

def null_test_top_confused_frac(
    per_embryo_metrics: pd.DataFrame,
    embryo_predictions: pd.DataFrame,
    n_permutations: int = 1_000,
    random_state: int = 42,
) -> pd.DataFrame:
    """
    Is the concentration of wrong predictions toward one class higher than
    expected by chance (i.e. dominant confusion rather than random noise)?

    Strategy:
      - Among wrong predictions for embryo i (true_class = C), pool all
        wrong-bin predictions for other embryos of the same true_class
      - Shuffle which class they point to (pool-and-redistribute within group)
      - Statistic = top_confused_frac (fraction pointing to argmax class)

    Uses: resample.permute_labels() on the confused-class column.

    Returns:
      pval_top_confused_frac
      qval_top_confused_frac
    """


def combine_pvals(
    per_embryo_pvals: List[pd.DataFrame],
    method: str = "fisher",
) -> pd.DataFrame:
    """
    Combine p-values from wrong_rate, streak, and top_confused_frac tests
    into a composite score per embryo.

    method: "fisher" | "simes" | "min" (bonferroni)

    Returns single DataFrame with all pval/qval columns + combined_pval + combined_qval.
    """


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
PART 4c — misclassification_flagging.py
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

def flag_consistently_misclassified(
    per_embryo_metrics: pd.DataFrame,  # includes pval/qval columns from Part 4b
    q_val_threshold: float = 0.05,
    wrong_rate_multiplier: float = 2.0,   # also flag if wrong_rate > 2x class baseline
    top_confused_frac_threshold: float = 0.8,   # flag dominant confusion to single class
    require_n_windows_min: int = 3,       # ignore embryos with < 3 time bins
) -> pd.DataFrame:
    """
    Flag embryos meeting any of:
      (A) qval_wrong_rate < threshold
      (B) qval_streak < threshold
      (C) wrong_rate > wrong_rate_multiplier * class_baseline AND n_windows >= min
      (D) top_confused_frac > threshold AND qval_top_confused_frac < threshold

    Returns flagged subset with 'flag_reasons' column (list of strings).
    """

def compute_confusion_enrichment(
    embryo_predictions: pd.DataFrame,
    flagged_embryos: pd.DataFrame,
) -> pd.DataFrame:
    """
    For each (true_class, top_confused_class) pair in flagged_embryos:
    Chi-square test: is this confusion pair enriched in flagged vs unflagged
    embryos of the same true_class?

    Returns DataFrame with:
      true_class | confused_as | n_flagged | n_unflagged |
      expected_frac | observed_frac | chi2 | pval | qval
    """

def compute_time_localization(
    embryo_predictions: pd.DataFrame,
    flagged_embryos: pd.DataFrame,
    rolling_window_bins: int = 3,
    rolling_threshold: float = 0.6,
) -> pd.DataFrame:
    """
    For each flagged embryo: when does misclassification begin and end?

    Uses rolling wrong-rate over rolling_window_bins consecutive bins.
    Onset = first bin where rolling_wrong_rate > rolling_threshold.
    Offset = last bin where rolling_wrong_rate > rolling_threshold.

    Returns:
      embryo_id | onset_time_bin | offset_time_bin |
      failure_phase (early/mid/late/sustained) |
      failure_duration_bins
    """


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
PART 4d — misclassification_viz.py
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

def plot_wrongness_heatmap(
    embryo_predictions: pd.DataFrame,
    per_embryo_metrics: pd.DataFrame,
    output_dir: Path,
    row_order: str = "wrong_rate",   # "wrong_rate" | "onset_time" | "cluster"
    cmap: str = "Reds",
    figsize: tuple = (14, 10),
) -> None:
    """
    Heatmap: rows=embryos, cols=time_bins, color=is_wrong (0/1) or p_true.
    Sidebar: true_class color strip, wrong_rate bar.
    Rows sorted by row_order.
    """

def plot_embryo_deep_dive(
    embryo_predictions: pd.DataFrame,
    embryo_id: str,
    output_dir: Path,
    class_colors: Optional[dict] = None,
) -> None:
    """
    Two-panel figure for one embryo:
      Top: bar/timeline of pred_label per time bin, colored by class, hatched if wrong
      Bottom: line plot of p_true and p_pred vs time_bin_center
    """

def plot_wrong_rate_distributions(
    per_embryo_metrics: pd.DataFrame,
    output_dir: Path,
    group_by: str = "true_class",
    show_flagged: bool = True,
) -> None:
    """
    Violin + strip plot of wrong_rate grouped by true_class.
    Flagged embryos shown as larger/different markers.
    """

def plot_confusion_profile(
    embryo_predictions: pd.DataFrame,
    flagged_embryos: pd.DataFrame,
    output_dir: Path,
) -> None:
    """
    For each true_class: stacked bar showing fraction of wrong bins going
    to each possible confused class (flagged embryos highlighted separately).
    """

def plot_flagged_embryo_gallery(
    embryo_predictions: pd.DataFrame,
    flagged_embryos: pd.DataFrame,
    output_dir: Path,
    top_n: int = 20,
) -> None:
    """
    Thin grid of deep-dive plots (one per flagged embryo, top_n by wrong_rate).
    Saves to output_dir/embryo_deep_dives/embryo_{id}.png
    """


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
PART 5 — OUTPUT FILE CONTRACT
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

results/mcolon/{timestamp}_classification_misclassification/
├── tables/
│   ├── embryo_predictions_augmented.parquet   ← pred_table with p_true, p_pred
│   ├── per_embryo_metrics.csv                 ← one row per embryo (Part 4a)
│   ├── per_embryo_null_pvals.csv              ← all pval/qval columns (Part 4b)
│   ├── flagged_embryos.csv                    ← subset with flag_reasons (Part 4c)
│   ├── confusion_enrichment.csv               ← (true_class, confused_as) stats
│   └── time_localization.csv                  ← onset/offset per flagged embryo
├── plots/
│   ├── wrongness_heatmap.png
│   ├── wrong_rate_distributions.png
│   ├── confusion_profile_by_class.png
│   └── embryo_deep_dives/
│       └── embryo_{id}.png                    ← top_n flagged only
└── metadata.json                              ← config + thresholds used


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
PART 6 — IMPLEMENTATION ORDER (MVP)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Step | Action                                          | Files touched
-----|--------------------------------------------------|-----------------------------
  1  | git mv difference_detection → classification    | directory rename
     | update __init__ imports                         | src/analyze/__init__.py
     | add deprecation shim                            | difference_detection/__init__.py
     | grep+fix downstream imports                     | results/, scripts/
-----|--------------------------------------------------|-----------------------------
  2  | Migrate _permutation_test_ovr() to              | classification_test_multiclass.py
     | resample.permute_labels(within="time_strata")   |
     | Add deprecation warnings to permutation_utils   | permutation_utils.py
-----|--------------------------------------------------|-----------------------------
  3  | Add p_true + p_pred to embryo_predictions       | classification_test_multiclass.py
     | (~5 lines in existing per-embryo loop)          |
-----|--------------------------------------------------|-----------------------------
  4  | Write misclassification_metrics.py              | new file
     | Write tests/test_misclassification_metrics.py   | new file
-----|--------------------------------------------------|-----------------------------
  5  | Write misclassification_null.py                 | new file
     | (wrong_rate + streak + top_confused_frac)       |
     | Write tests/test_misclassification_null.py      | new file
-----|--------------------------------------------------|-----------------------------
  6  | Write misclassification_flagging.py             | new file
     | (flag_consistently_misclassified,               |
     |  compute_confusion_enrichment,                  |
     |  compute_time_localization)                     |
-----|--------------------------------------------------|-----------------------------
  7  | Write misclassification_viz.py                  | new file
     | (heatmap, deep_dive, distributions,             |
     |  confusion_profile, gallery)                    |
-----|--------------------------------------------------|-----------------------------
  8  | Update classification/__init__.py exports       | classification/__init__.py
     | (expose new public functions)                   |
-----|--------------------------------------------------|-----------------------------
  9  | Commit + push to claude/add-misclassification-  | git
     | results-JVNdh                                   |


━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
KEY DESIGN DECISIONS
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

1. Flat file layout (not subpackage)
   All new files live directly in classification/, not in a misclassification/
   subpackage. This matches the existing flat structure.

2. New resampling framework (not permutation_utils)
   All permutation loops use resample.permute_labels() / resample.run().
   permutation_utils.py is kept with deprecation warnings, not deleted,
   so external callers have time to migrate.

3. p_true and p_pred added in-place
   These are pre-computed convenience columns on embryo_predictions rather
   than a separate join, because the classifier already has probs in scope
   at that point (zero extra compute).

4. BH FDR correction is done WITHIN each test (not combined)
   Each of the three null tests (wrong_rate, streak, top_confused_frac)
   produces its own BH-corrected q-value. combine_pvals() is a separate
   optional function for users who want a joint composite score.

5. Bootstrap consistency (replicate-level) is deferred
   Running run_classification_test() N times on subsampled embryos is a
   large engineering task (requires stable data splits). It goes in a
   future PR and is NOT in scope here.

6. resample.permute_labels(within="time_bin") for wrong_rate null
   This shuffles which pred_label each embryo gets within each time bin,
   preserving the per-bin class distribution. This matches the stratified
   permutation philosophy already in _permutation_test_ovr().

================================================================================
END OF PLAN
================================================================================
