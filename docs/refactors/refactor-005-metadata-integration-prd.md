# Refactor PRD 005: Fundamental Metadata Integration for Legacy Compatibility


## 1. Objective

This document outlines a critical architectural refactoring aimed at resolving a fundamental data incompatibility discovered during the integration of the SAM2 segmentation pipeline with the legacy `morphseq` processing. The objective is to ensure that essential raw image metadata, specifically physical dimensions (`Height (um)`) and pixel dimensions (`Height (px)`), are accurately and directly propagated through the SAM2 pipeline's metadata system, thereby enabling seamless compatibility with downstream legacy analysis.

## 2. Background: The Fundamental Incompatibility

During the refactoring efforts detailed in previous PRDs (001-004), a core incompatibility was identified:

*   **Legacy Pipeline:** The original `morphseq` pipeline relied on direct access to `Height (um)` and `Height (px)` of the raw image files. These values were extracted early (e.g., by `build01A_compile_keyence_torch.py` and `build01B_compile_yx1_images_torch.py`) and propagated through its metadata system. This allowed for precise physical scaling of image snips.
*   **SAM2 Pipeline (Current Integration):** The `segmentation_sandbox`'s SAM2 pipeline, as currently integrated, does not carry forward these specific raw image dimensions. Its `experiment_metadata.json` and `GroundedSam2Annotations.json` primarily store pixel dimensions of *processed images* that the SAM2 models operate on, not the original raw image physical dimensions. This necessitated the use of empirically derived formulas (e.g., `(rs_factor / 3.648) * 6.5` for `px_dim_raw`) to bridge this information gap, introducing a "magic number" and a less direct data flow.

This gap represents a breakdown in information lineage, where crucial raw image properties are lost or abstracted away, leading to complex workarounds.

## 3. Proposed Solution: Direct Metadata Integration

The proposed solution is to directly integrate the raw image `Height (um)` and `Height (px)` into the `segmentation_sandbox`'s central `experiment_metadata.json` object at the earliest possible stage. This will ensure this critical information is available and directly accessible throughout the pipeline.

### 3.1. Strategy Overview

The strategy involves modifying the initial data organization stage of the SAM2 pipeline to extract and store the raw image dimensions, and then ensuring this information is correctly aggregated for downstream use.

### 3.2. Implementation Steps

1.  **Enhance `scripts/data_organization/data_organizer.py`:**

    *   Modify this script to load the existing metadata files generated by earlier legacy build scripts (e.g., `metadata/built_metadata_files/{experiment_date}_metadata.csv`). These files already contain the `Height (um)` and `Height (px)` information sourced from the raw image files.
    *   For each `image_id` it processes (derived from the stitched image filenames), `data_organizer.py` will parse out the `well_id` and `time_int` (frame number).
    *   It will then perform a lookup in the loaded legacy metadata CSVs using these `well_id` and `time_int` to retrieve the corresponding `Height (um)` and `Height (px)`.
    *   Store this retrieved `Height (um)` and `Height (px)` information within the `experiment_metadata.json` structure, specifically at the `image_ids` level for each processed image.

2.  **Update `segmentation_sandbox/scripts/utils/export_sam2_metadata_to_csv.py`:**
    *   Modify this metadata bridge script to read *both* the `GroundedSam2Annotations.json` (for segmentation results) *and* the `experiment_metadata.json` (for the newly added `Height (um)` and `Height (px)`).
    *   Merge this information based on common identifiers (e.g., `image_id`, `video_id`, `experiment_id`) to produce a comprehensive `sam2_metadata.csv` that includes the raw image physical dimensions.

3.  **Simplify `src/build/build03A_process_images.py`:**
    *   Once `Height (um)` and `Height (px)` are directly available in the `sam2_metadata.csv`, the `segment_wells_sam2_csv` function can be simplified.
    *   The empirically derived `px_dim_raw` formula can be replaced with a direct calculation: `row['px_dim_raw'] = row['Height (um)'] / row['Height (px)']`.

### 3.3. Detailed Metadata Schema and Integration Plan

Based on the analysis of `test_data/sample_built_metadata.csv` (a representative file from `metadata/built_metadata_files/`), the following insights are crucial for the integration:

**Content of `built_metadata_files/{experiment_date}_metadata.csv`:**

This CSV contains per-well-time-point metadata, including:
*   `well_id`, `well`, `nd2_series_num`, `microscope`, `time_int`, `Height (um)`, `Width (um)`, `Height (px)`, `Width (px)`, `BF Channel`, `Objective`, `Time (s)`, `experiment_date`, `medium`, `genotype`, `chem_perturbation`, `start_age_hpf`, `embryos_per_well`, `temperature`, `well_qc_flag`, `Time Rel (s)`.

**Proposed `experiment_metadata.json` Schema Modification:**

To effectively integrate this information without redundancy and maintain logical structure, the `image_ids` field within each `video` object in `experiment_metadata.json` must undergo a significant schema change.

*   **Current `image_ids` (List of Strings):**
    ```json
    "image_ids": [
        "20240411_A01_t0000",
        "20240411_A01_t0001"
    ]
    ```
*   **Proposed `image_ids` (Dictionary of Objects, keyed by `image_id`):**
    ```json
    "image_ids": {
        "20240418_A01_t0000": {
            "frame_index": 0, // Existing field, confirm consistency
            // Raw Image Dimensions (Crucial for scaling)
            "raw_height_um": 7080.86262190448,
            "raw_width_um": 7080.86262190448,
            "raw_height_px": 2189,
            "raw_width_px": 2189,
            // Acquisition Parameters (Useful for context/filtering)
            "microscope": "YX1",
            "objective": "Plan Apo Î» 4x",
            "bf_channel": 0,
            "nd2_series_num": 1, // Specific to YX1, useful for tracing raw file
            // Time Information (Useful for analysis)
            "raw_time_s": 0.9772776351561769, // Original time from microscope
            "relative_time_s": 0.0, // Time relative to experiment start
            // ... other relevant fields from the CSV that are image-specific
        },
        "20240418_A01_t0001": { /* ... similar structure for other images ... */ }
    }
    ```

**Handling Well-Level Metadata:**

Columns from `built_metadata_files/{experiment_date}_metadata.csv` that represent **well-level metadata** (i.e., constant for all images within a given well/video) should ideally be stored at the `video` object level within `experiment_metadata.json` to avoid redundancy. These include:

*   `medium`
*   `genotype`
*   `chem_perturbation`
*   `start_age_hpf`
*   `embryos_per_well`
*   `temperature`
*   `well_qc_flag`

**Impact on Downstream Scripts:**

This change to the `experiment_metadata.json` schema is **significant**. All scripts that read or write to `experiment_metadata.json` (e.g., `data_organizer.py`, `gdino_detection.py`, `sam2_segmentation.py`, `export_sam2_metadata_to_csv.py`) will need to be updated to expect `image_ids` as a dictionary of objects, not a list of strings. This represents a major refactoring effort across the `segmentation_sandbox` pipeline.

### 3.4. Consistency Across Microscope Data Sources
 Key Similarities:

   * Modular Design: Both scripts are structured with functions for distinct tasks (e.g., initial processing, stitching).
   * Metadata-Driven: Both collect and manage metadata using Pandas DataFrames.
   * Image-Level Granularity: Crucially, both process images at a granular level (well/time-point/position) and extract metadata for each. This directly maps to the image_id concept in
     the SAM2 pipeline.
   * Common Output: Both save their primary metadata output to metadata/built_metadata_files/{experiment_date}_metadata.csv. This is the consistent source we can leverage.
   * Stitched FF Image Output: Both produce stitched full-focus (FF) images, which are the inputs for later stages.

  Key Differences (Keyence Specifics):

   * Metadata Sourcing: Keyence uses scrape_keyence_metadata (which parses embedded XML metadata from raw Keyence files) to extract Height (um), Width (um), Height (px), Width (px),
     Objective, Time (s), and Channel. YX1 uses the nd2 library for similar information.
   * Focus Stacking: Keyence uses doLap for Laplacian focus stacking, while YX1 uses LoG_focus_stacker.
   * Stitching: Keyence explicitly performs stitching of individual image tiles using stitch2d.StructuredMosaic within stitch_experiment. YX1's build_ff_from_yx1 processes already
     organized ND2 files, implying stitching for YX1 would be a separate upstream step if multiple fields of view per well were acquired.

  Confirmation of Image-Level Data Creation:

  Yes, the metadata is indeed generated at the image level (or well/time-point/position level), which directly corresponds to the image_id in the SAM2 pipeline's experiment_metadata.json.
   For each unique image (well/time-point/position), a row of metadata is generated, including the physical and pixel dimensions.

  Reinforcing `image_id` Storage for Simplicity:

  Given that both Keyence and YX1 build scripts consistently produce this rich metadata at the image level, storing all relevant information directly within the image_id object in
  experiment_metadata.json is absolutely the most straightforward and simple approach for implementation. This aligns perfectly with the schema change proposed in
  refactor-005-metadata-integration-prd.md. It avoids complex lookups, redundant storage, and maintains a clear, self-contained record for each image.
  
## 4. Benefits

*   **Elimination of "Magic Numbers":** Removes the reliance on empirically derived constants like `3.648`, leading to a more transparent and robust scaling mechanism.
*   **Improved Data Lineage:** Ensures that critical raw image properties are explicitly carried forward from the earliest stage of the SAM2 pipeline.
*   **Enhanced Accuracy:** Direct sourcing of physical dimensions reduces potential for errors or inconsistencies introduced by estimation.
*   **Cleaner Codebase:** Simplifies downstream calculations in `build03A_process_images.py` by providing direct access to necessary metadata.
*   **Architectural Consistency:** Aligns the SAM2 pipeline's metadata handling more closely with the principles of comprehensive data tracking.

## 5. Reference to Previous Refactoring PRDs

This refactoring effort builds upon the insights and challenges encountered in previous stages. For context on the fundamental incompatibility and the evolution of the integration strategy, please refer to:

*   `docs/refactors/refactor-001-segmentation-pipeline-integration-prd.md`
*   `docs/refactors/refactor-002-segmentation-pipeline-integration-prd.md`
*   `docs/refactors/refactor-003-segmentation-pipeline-integration-prd.md`
*   `docs/refactors/refactor-004-debugging-and-stabilization.md` (if applicable, assuming this exists or is planned)
