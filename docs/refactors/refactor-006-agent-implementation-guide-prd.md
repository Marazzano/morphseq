# Refactor PRD 006: Agent Implementation Guide for Metadata Integration

## 1. Objective

This document provides a comprehensive guide for an agent to implement Refactor 005 (Fundamental Metadata Integration) with full context of the refactoring evolution from PRDs 001-005. The objective is to complete the schema enhancement that eliminates magic numbers by integrating raw image metadata directly into the SAM2 pipeline.

## 2. Refactoring Evolution & Context

### 2.1. Historical Context: How We Got Here

The SAM2 integration has evolved through five major phases, each building on lessons learned:

**PRD 001 (MVP Integration):**
- **Focus**: Simple mask format detection and backward compatibility
- **Approach**: Band-aid solution with mask format detector utility
- **Key Insight**: Direct mask loading worked, but didn't address core tracking complexity
- **Status**: Basic integration achieved but system remained complex

**PRD 002 (Full Pipeline Integration):**
- **Focus**: Complete removal of `region_label` tracking system
- **Approach**: Surgical replacement of tracking functions with SAM2 integer masks
- **Key Insight**: SAM2's instance-aware masks eliminate need for fragile tracking
- **Status**: Major architecture improvement, but build scripts still did redundant work

**PRD 003 (Metadata Bridge Architecture):**
- **Focus**: Transform build scripts from data-processors to data-consumers
- **Approach**: Bridge script flattens SAM2 JSON to CSV, builds consume pre-computed metadata
- **Key Insight**: Most efficient architecture eliminates redundant calculations entirely
- **Status**: ‚úÖ Phase 1 (bridge script) completed, Phase 2 (build integration) with critical regression resolved
- **Critical Discovery**: Found and fixed major scaling regression that was producing empty/black masks

**PRD 004 (Debugging & Stabilization):**
- **Focus**: End-to-end pipeline stability and bug resolution
- **Approach**: Comprehensive debugging of conda environment, file paths, data types, rotation logic
- **Key Insight**: System needed extensive debugging for production stability
- **Status**: ‚úÖ Functional end-to-end pipeline achieved, ready for full dataset

**PRD 005 (Metadata Integration - Current):**
- **Focus**: Eliminate magic numbers by integrating raw image dimensions into SAM2 pipeline
- **Approach**: Complete schema transformation of `experiment_metadata.json`
- **Key Issue Discovered**: SAM2 pipeline loses raw image `Height (um)` and `Height (px)`, forcing empirical formula `(rs_factor / 3.648) * 6.5`
- **Status**: ‚è≥ Ready for implementation

### 2.2. Current Problem Definition

**The Magic Number Crisis:**
The current SAM2 integration uses an empirically derived formula:
```python
row['px_dim_raw'] = (rs_factor / 3.648) * 6.5
```

This magic number `3.648` exists because:
1. **Legacy Pipeline**: Directly accesses `Height (um)` and `Height (px)` from raw image metadata
2. **SAM2 Pipeline**: Only stores processed image dimensions, not raw image physical dimensions
3. **Gap**: Critical raw image properties are lost, requiring empirical workarounds

**The Solution:**
Integrate raw image metadata from legacy CSV files (`metadata/built_metadata_files/{experiment_date}_metadata.csv`) directly into the SAM2 pipeline's `experiment_metadata.json`, enabling direct calculation: `row['px_dim_raw'] = row['Height (um)'] / row['Height (px)']`

## 3. Complete Implementation Plan

### Phase 1: Data Organizer Schema Transformation ‚úÖ APPROVED

**File**: `segmentation_sandbox/scripts/data_organization/data_organizer.py`
**Target Methods**: `scan_video_directory()`, `scan_organized_experiments()`

**Agent Task**: Implement the complete enhanced schema from PRD 005:

#### 3.1. Schema Enhancement Details

**Current Schema** (simple):
```json
{
  "videos": {
    "20240418_A01": {
      "video_id": "20240418_A01",
      "image_ids": ["20240418_A01_t0000", "20240418_A01_t0001"]
    }
  }
}
```

**Enhanced Schema** (target):
```json
{
  "experiments": {
    "20240418": {
      "videos": {
        "20240418_A01": {
          "video_id": "20240418_A01",
          "well_id": "A01",
          "mp4_path": "...",
          "processed_jpg_images_dir": "...",
          "total_frames": 123,
          "image_size": [H, W],
          "source_well_metadata_csv": "metadata/built_metadata_files/20240418_metadata.csv",
          
          // Well-level metadata from CSV (constant per well)
          "medium": "E3",
          "genotype": "wildtype",
          "chem_perturbation": "none", 
          "start_age_hpf": 24,
          "embryos_per_well": 1,
          "temperature": 28.5,
          "well_qc_flag": 0,
            //... and any other video level info from orignal gsam pipline and the build scripts amasses at this stage
          
          // BREAKING CHANGE: image_ids as dictionary
          "image_ids": {
            "20240418_A01_ch00_t0000": { //note parsing_id_convention.md
              "frame_index": 0,
              "raw_image_data_info": {
                "Height (um)": 7080.86,
                "Height (px)": 2189,
                "Width (um)": 7080.86, 
                "Width (px)": 2189,
                "microscope": "YX1",
                "objective": "Plan Apo Œª 4x",
                "channel": 0, 
                "nd2_series_num": 1,
                "raw_time_s": 0.977,
                "relative_time_s": 0.0,
                "stitched_image_path": "/path/to/stitched.jpg"
              }
            }
          }
        }
      }
    }
  }
}
```

#### 3.2. Critical Schema Design Principles

**1. ID Convention Compliance:**
- Must follow `parsing_id_convention.md` standards
- Use format: `"20240418_A01_ch00_t0000"` (includes channel designation)
- Ensures consistency across entire segmentation_sandbox pipeline

**2. Column Name Strategy:**
- **Preserve Original**: Keep exact CSV column names like `"Height (um)"` for data lineage
- **Add Aliases**: Include code-friendly versions like `"height_um"` for easy access
- **Avoid Fragile Access**: No need for `row['Height (um)']` bracket notation with spaces

**3. Dimension Disambiguation:**
- **`processed_image_size_px`**: Dimensions SAM2 pipeline works with (e.g., [1024, 1024])
- **`height_px/width_px` in raw_image_data_info**: Original microscope capture dimensions (e.g., 2189)
- **Clear Separation**: Eliminates confusion between processed vs raw image dimensions

#### 3.3. Agent Implementation Steps

**Step 1: Locate Target Functions**
```python
# Find these key functions in data_organizer.py:
def scan_video_directory(...)  # Processes individual video directories
def scan_organized_experiments(...)  # Main orchestrator function
```

**Step 2: Add CSV Loading Logic**
```python
def load_legacy_metadata_csv(experiment_date):
    """Load raw image metadata from legacy build scripts"""
    csv_path = f"metadata/built_metadata_files/{experiment_date}_metadata.csv"
    if os.path.exists(csv_path):
        return pd.read_csv(csv_path)
    else:
        logger.warning(f"Legacy metadata CSV not found: {csv_path}")
        return None
```

**Step 3: Parse Video ID for CSV Lookup**
```python
def parse_video_id_for_metadata(video_id):
    """Extract experiment_date and well_id from video_id"""
    # video_id format: "20240418_A01_ch00" (following parsing_id_convention.md)
    parts = video_id.split('_')
    experiment_date = parts[0]  # "20240418"
    well_id = parts[1]         # "A01" 
    channel = parts[2] if len(parts) > 2 else "ch00"  # "ch00"
    return experiment_date, well_id, channel
```

**Step 4: Schema Transformation Logic**
```python
def enhance_video_metadata_with_csv(video_data, csv_df, well_id):
    """Transform video metadata to enhanced schema"""
    
    # 1. Add well-level metadata (constant per well)
    well_rows = csv_df[csv_df['well_id'] == well_id]
    if not well_rows.empty:
        first_row = well_rows.iloc[0]
        video_data.update({
            'well_id': well_id,
            'source_well_metadata_csv': csv_path,
            'medium': first_row['medium'],
            'genotype': first_row['genotype'],
            'chem_perturbation': first_row['chem_perturbation'],
            'start_age_hpf': first_row['start_age_hpf'],
            'embryos_per_well': first_row['embryos_per_well'],
            'temperature': first_row['temperature'],
            'well_qc_flag': first_row['well_qc_flag']
        })
    
    # 2. Transform image_ids from list to dictionary
    old_image_ids = video_data.get('image_ids', [])
    new_image_ids = {}
    
    for i, image_id in enumerate(old_image_ids):
        # Extract time_int from image_id (e.g., "20240418_A01_ch00_t0000" -> 0)
        time_int = int(image_id.split('_t')[-1])
        
        # Find matching CSV row for this well_id + time_int
        matching_rows = csv_df[(csv_df['well_id'] == well_id) & 
                              (csv_df['time_int'] == time_int)]
        
        image_info = {
            'frame_index': i,
            'raw_image_data_info': {}
        }
        
        if not matching_rows.empty:
            row = matching_rows.iloc[0]
            image_info['raw_image_data_info'] = {
                # Original CSV column names (preserves data lineage)
                'Height (um)': row['Height (um)'],
                'Height (px)': row['Height (px)'],
                'Width (um)': row['Width (um)'],
                'Width (px)': row['Width (px)'],
                'BF Channel': row['BF Channel'],
                'Objective': row['Objective'],
                'Time (s)': row['Time (s)'],
                'Time Rel (s)': row['Time Rel (s)'],
                
                # Code-friendly aliases (avoids fragile dict access)
                'height_um': row['Height (um)'],
                'height_px': row['Height (px)'],
                'width_um': row['Width (um)'],
                'width_px': row['Width (px)'],
                'bf_channel': row['BF Channel'],
                'objective': row['Objective'],
                'raw_time_s': row['Time (s)'],
                'relative_time_s': row['Time Rel (s)'],
                
                # Additional metadata
                'microscope': row['microscope'],
                'nd2_series_num': row.get('nd2_series_num', None),
                'stitched_image_path': f"/path/to/stitched_FF_images/{experiment_date}/ff_{well_id}_t{time_int:04d}.jpg"
            }
        
        new_image_ids[image_id] = image_info
    
    # 3. Replace image_ids list with dictionary
    video_data['image_ids'] = new_image_ids
    
    return video_data
```

**Step 5: Integration into Main Functions**
```python
def scan_video_directory(video_dir_path, ...):
    # ... existing logic ...
    
    # NEW: Enhance with CSV metadata
    experiment_date, well_id, channel = parse_video_id_for_metadata(video_id)
    csv_df = load_legacy_metadata_csv(experiment_date)
    
    if csv_df is not None:
        video_data = enhance_video_metadata_with_csv(video_data, csv_df, well_id)
        
        # Update processed vs raw dimension clarity
        if 'image_size' in video_data:
            video_data['processed_image_size_px'] = video_data.pop('image_size')
    
    # ... rest of function ...
```

### Phase 2: Update ALL Affected Scripts (CRITICAL BREAKING CHANGE)

**Impact**: The `image_ids` structure change from list to dictionary affects ALL segmentation_sandbox scripts

#### 2.1. Scripts Requiring Updates

**Agent Task**: Systematically update every script that accesses `image_ids` in `experiment_metadata.json`

**Primary Scripts**:
1. `segmentation_sandbox/scripts/detection_segmentation/gdino_detection.py`
2. `segmentation_sandbox/scripts/detection_segmentation/sam2_segmentation.py` 
3. `segmentation_sandbox/scripts/utils/export_sam2_metadata_to_csv.py`

**Search Pattern for Agent**:
```bash
# Use Grep tool to find all files accessing "image_ids"
grep -r "image_ids" segmentation_sandbox/scripts/ --include="*.py"
```

#### 2.2. Code Pattern Updates

**Old Pattern** (list iteration):
```python
# OLD: image_ids as list
for image_id in video_data["image_ids"]:
    process_image(image_id)
```

**New Pattern** (dictionary iteration):  
```python
# NEW: image_ids as dictionary
for image_id, image_info in video_data["image_ids"].items():
    frame_index = image_info["frame_index"]
    raw_data = image_info["raw_image_data_info"] 
    process_image(image_id, frame_index, raw_data)
```

#### 2.3. Special Focus: export_sam2_metadata_to_csv.py

**Critical Enhancement**: This script must merge segmentation data with raw image metadata

**Agent Implementation**:
```python
def enhance_csv_with_raw_metadata(sam2_csv_df, experiment_metadata):
    """Add raw image metadata columns to SAM2 CSV"""
    
    enhanced_rows = []
    for _, row in sam2_csv_df.iterrows():
        enhanced_row = row.copy()
        
        # Extract video_id from image_id
        image_id = row['image_id']
        video_id = '_'.join(image_id.split('_')[:2])  # "20240418_A01_t0000" -> "20240418_A01"
        experiment_id = video_id.split('_')[0]       # "20240418"
        
        # Look up raw metadata
        if experiment_id in experiment_metadata['experiments']:
            exp_data = experiment_metadata['experiments'][experiment_id]
            if 'videos' in exp_data and video_id in exp_data['videos']:
                video_data = exp_data['videos'][video_id]
                
                # Add well-level metadata
                for key in ['medium', 'genotype', 'chem_perturbation', 
                           'start_age_hpf', 'embryos_per_well', 'temperature', 'well_qc_flag']:
                    enhanced_row[key] = video_data.get(key, None)
                
                # Add image-level raw metadata
                if 'image_ids' in video_data and image_id in video_data['image_ids']:
                    raw_data = video_data['image_ids'][image_id]['raw_image_data_info']
                    for key, value in raw_data.items():
                        enhanced_row[key] = value
        
        enhanced_rows.append(enhanced_row)
    
    return pd.DataFrame(enhanced_rows)
```

### Phase 3: Simplify build03A_process_images.py

**File**: `src/build/build03A_process_images.py`
**Function**: `segment_wells_sam2_csv()`

**Agent Task**: Replace magic number formula

**Current Code** (contains magic number):
```python
row['px_dim_raw'] = (rs_factor / 3.648) * 6.5
```

**Target Code Options** (direct calculation):
```python
# Option 1: Use original CSV column names (preserves exact lineage)
row['px_dim_raw'] = row['Height (um)'] / row['Height (px)']

# Option 2: Use code-friendly aliases (cleaner syntax)
row['px_dim_raw'] = row['height_um'] / row['height_px']
```

**Recommendation**: Use Option 2 (aliases) for cleaner code, but verify both columns exist in enhanced CSV.

**Agent Steps**:
1. **Search for Magic Number**: Find `(rs_factor / 3.648) * 6.5` in build03A_process_images.py
2. **Verify CSV Enhancement**: Ensure Phase 2 produced CSV with both original names AND aliases
3. **Replace Calculation**: Update to direct calculation using aliases
4. **Test**: Verify no calculation errors or missing columns

## 4. Agent Capabilities Required

### 4.1. Technical Skills Needed
1. **File Reading/Writing**: Read, Edit, MultiEdit tools for Python files
2. **Code Analysis**: Grep tool for pattern searching across codebase  
3. **JSON Schema Understanding**: Nested JSON structure manipulation
4. **CSV Data Processing**: pandas DataFrame operations
5. **Python Function Modification**: Edit function logic and data structures

### 4.2. Key Understanding Points
1. **Breaking Change Nature**: This is a major schema transformation, not additive
2. **Data Flow**: Raw CSV ‚Üí enhanced JSON ‚Üí enhanced CSV ‚Üí build scripts
3. **Column Name Preservation**: Must keep exact CSV column names like `"Height (um)"`
4. **Error Handling**: Graceful fallbacks for missing CSV files or data
5. **Legacy Compatibility**: All existing functionality must be preserved

## 5. Agent Implementation Strategy

### 5.1. Systematic Approach

**Phase 1 Execution**:
```
1. Read data_organizer.py to understand current structure
2. Implement CSV loading utilities
3. Add video_id parsing functions  
4. Implement schema transformation logic
5. Update main scan functions to use enhancement
6. Test with sample experiment data
```

**Phase 2 Execution**:
```
1. Use Grep to find all files accessing "image_ids"
2. For each file found:
   a. Read and analyze current usage
   b. Update iteration patterns from list to dictionary
   c. Ensure access to frame_index and raw_image_data_info
   d. Test changes don't break functionality
3. Special handling for export_sam2_metadata_to_csv.py:
   a. Add raw metadata merge logic
   b. Verify output CSV has all required columns
```

**Phase 3 Execution**:
```
1. Search for magic number formula in build03A_process_images.py
2. Verify enhanced CSV contains Height columns  
3. Replace with direct calculation
4. Test calculation works without errors
```

### 5.2. Validation & Testing

**Agent Verification Steps**:
1. **Schema Validation**: Generate sample enhanced experiment_metadata.json
2. **Script Compatibility**: Test all modified scripts read enhanced format
3. **CSV Enhancement**: Verify export script produces comprehensive CSV
4. **Magic Number Elimination**: Confirm build03A uses direct calculation
5. **No Regression**: Ensure no existing functionality broken

**Success Criteria**:
- Enhanced experiment_metadata.json with complete dictionary structure
- All segmentation_sandbox scripts compatible with new schema
- Comprehensive CSV output with raw metadata columns
- build03A eliminates magic number, uses direct calculation
- Complete metadata lineage: legacy CSV ‚Üí SAM2 pipeline ‚Üí build scripts

## 6. Critical Files for Agent Reference

### 6.1. Implementation Files
- **`segmentation_sandbox/scripts/data_organization/data_organizer.py`** - Primary target for Phase 1
- **`segmentation_sandbox/scripts/utils/export_sam2_metadata_to_csv.py`** - CSV enhancement for Phase 2  
- **`src/build/build03A_process_images.py`** - Magic number elimination for Phase 3

### 6.2. Reference Documentation
- **`docs/refactors/refactor-005-metadata-integration-prd.md`** - Complete enhanced schema specification
- **`segmentation_sandbox/docs/data_structure_segmentation_sandbox_overview.md`** - Current JSON schemas
- **`docs/legacy_pipeline_overview.md`** - Legacy metadata sourcing details

### 6.3. Sample Data
- **`metadata/built_metadata_files/{experiment_date}_metadata.csv`** - Raw metadata source
- **`test_data/sample_built_metadata.csv`** - Sample for testing (if available)

## 7. Expected Outcomes

### 7.1. Immediate Benefits
1. **Magic Number Elimination**: Complete removal of empirical constant `3.648`
2. **Data Lineage Improvement**: Direct trace from raw images to final calculations  
3. **Enhanced Accuracy**: Physical scaling based on actual measurements, not estimates
4. **Code Simplification**: Cleaner, more transparent calculation logic

### 7.2. Architectural Benefits
1. **Metadata Completeness**: SAM2 pipeline gains full raw image metadata
2. **Schema Consistency**: All entity metadata stored in structured, accessible format
3. **Integration Robustness**: Less dependency on empirical workarounds
4. **Future Extensibility**: Enhanced schema supports additional metadata as needed

### 7.3. Risk Mitigation
1. **Backward Compatibility**: Graceful handling when CSV files missing
2. **Error Handling**: Clear warnings and fallbacks for incomplete data
3. **Validation**: Schema validation ensures JSON structure integrity
4. **Testing**: Comprehensive verification at each implementation phase

## 8. Conclusion

This refactor represents the culmination of a sophisticated evolution in SAM2 integration. Building on the foundation of successful mask integration (PRDs 001-004), this final phase eliminates the last major architectural gap: the loss of raw image metadata that forced reliance on magic numbers.

The agent implementing this plan will complete the transformation from a fragile, tracking-dependent system to a robust, metadata-driven architecture that preserves complete data lineage from raw microscope images through to final analysis outputs.

---

# üìã **ACTUAL IMPLEMENTATION STATUS** - Updated 2025-08-29

## **REFACTOR 005 CURRENT STATE: PARTIALLY IMPLEMENTED**

**Critical Discovery**: Previous claims of complete implementation were inaccurate. Here is the actual current state after thorough investigation:

### **‚úÖ ACTUALLY IMPLEMENTED (PHASE 1)**

**File**: `segmentation_sandbox/scripts/data_organization/data_organizer_refactor_test.py`

**Successfully Working**:
- ‚úÖ `load_legacy_metadata_csv()` - Loads raw image metadata from `/net/trapnell/vol1/home/nlammers/projects/data/morphseq/metadata/built_metadata_files/{experiment_name}_metadata.csv`
- ‚úÖ `parse_video_id_for_metadata()` - Extracts experiment_name, well_id, and channel from video IDs
- ‚úÖ `enhance_video_metadata_with_csv()` - Complete schema transformation function with JSON serialization fixes
- ‚úÖ `convert_to_json_serializable()` - Handles pandas/numpy data types for JSON serialization
- ‚úÖ **Schema Transformation**: `image_ids` converted from list ‚Üí dictionary format
- ‚úÖ **Raw Metadata Integration**: Added `raw_image_data_info` with both:
  - Original CSV column names (`"Height (um)"`, `"Width (um)"`) for data lineage preservation
  - Code-friendly aliases (`"height_um"`, `"width_um"`) for clean programmatic access
- ‚úÖ **Well-level Metadata**: Added `medium`, `genotype`, `chem_perturbation`, `start_age_hpf`, etc.
- ‚úÖ **Enhanced Processing**: Successfully processes experiments like `20250612_30hpf_ctrl_atf6`

**Validation Results from Real Testing**:
- ‚úÖ CSV loading working: `/net/trapnell/vol1/home/nlammers/projects/data/morphseq/metadata/built_metadata_files/20250612_30hpf_ctrl_atf6_metadata.csv`
- ‚úÖ Schema transformation successful: Dictionary `image_ids` with `raw_image_data_info`
- ‚úÖ Well metadata integration: `medium`, `genotype`, experimental conditions
- ‚úÖ JSON serialization: Fixed pandas/numpy type conversion issues

### **‚ùå FALSELY CLAIMED AS COMPLETE (PHASE 2)**

**These files were NOT actually modified despite previous claims**:

**`segmentation_sandbox/scripts/metadata/experiment_metadata.py`**: 
- ‚ùå **No modifications made** - Script still expects list format for `image_ids`
- ‚ùå **No backwards compatibility** - Will break with dictionary format
- ‚ùå **Functions not enhanced**: `add_images_to_video()`, `get_image()`, `list_images()` unchanged

**`segmentation_sandbox/scripts/utils/export_sam2_metadata_to_csv.py`**: 
- ‚ùå **No 39-column enhancement** - Still exports original 14-column format
- ‚ùå **No raw metadata integration** - Does not access `raw_image_data_info`
- ‚ùå **No well-level metadata** - Missing experimental context columns

**Detection/Segmentation Scripts**:
- ‚ùå `segmentation_sandbox/scripts/detection_segmentation/gdino_detection.py` - Not updated
- ‚ùå `segmentation_sandbox/scripts/detection_segmentation/sam2_segmentation.py` - Not updated

### **‚úÖ PHASE 3: Magic Number Status**

**File Checked**: `src/build/build03A_process_images.py`

**Status**: ‚úÖ **ALREADY CORRECT** - Uses direct calculation:
```python
px_dim_raw = row["Height (um)"] / row["Height (px)"]
```

**No empirical formula found** - Magic number elimination already achieved.

---

## **üéØ ACTUAL CURRENT STATE**

### **What Actually Works**:
1. ‚úÖ **Enhanced Schema Generation**: `data_organizer_refactor_test.py` creates proper dictionary format
2. ‚úÖ **CSV Metadata Integration**: Successfully loads and integrates legacy metadata
3. ‚úÖ **JSON Serialization**: Handles pandas/numpy types correctly
4. ‚úÖ **Raw Image Data**: Both original column names and aliases preserved

### **What Needs Implementation**:
1. ‚ùå **Pipeline Script Updates**: All scripts expecting list `image_ids` format
2. ‚ùå **Enhanced CSV Export**: Need 39-column export with raw metadata
3. ‚ùå **Backwards Compatibility**: Scripts must handle both formats
4. ‚ùå **Production Integration**: Move from `_refactor_test.py` to production files

### **Critical Breaking Changes**:
- **Dictionary Format**: `image_ids` is now `{image_id: {frame_index, raw_image_data_info}}`
- **Pipeline Incompatibility**: Existing scripts will break with new format
- **Two-Phase Rollout Needed**: Must update all consuming scripts before production use

---

## **üöÄ REMAINING TASKS FOR NEXT AGENT**

### **IMMEDIATE (HIGH PRIORITY)**

1. **Clean Up Working Implementation**:
   - Remove debug code from `data_organizer_refactor_test.py`
   - Migrate working code to production `data_organizer.py`

2. **Update Pipeline Scripts** (Breaking Changes):
   ```bash
   # Find all scripts that access image_ids
   grep -r "image_ids" segmentation_sandbox/scripts/ --include="*.py"
   ```
   
   **Must Update These Files**:
   - `experiment_metadata.py` - Add backwards compatibility
   - `gdino_detection.py` - Handle dictionary format
   - `sam2_segmentation.py` - Handle dictionary format
   - `export_sam2_metadata_to_csv.py` - Add 39-column enhanced export

3. **Implement Enhanced CSV Export**:
   - Add raw metadata columns (18 new columns)
   - Add well-level metadata (7 new columns)  
   - Preserve both original names and aliases
   - Total: 14 ‚Üí 39 columns

### **TESTING & VALIDATION (MEDIUM PRIORITY)**

1. **End-to-End Pipeline Test**:
   ```bash
   # Test with enhanced metadata
   python data_organizer.py  # Generate enhanced experiment_metadata.json
   python gdino_detection.py  # Verify dictionary format compatibility
   python sam2_segmentation.py  # Verify segmentation with new format
   python export_sam2_metadata_to_csv.py  # Generate 39-column CSV
   ```

2. **Backwards Compatibility Test**:
   - Ensure existing list-format metadata still works
   - Verify gradual migration path

### **PRODUCTION READINESS (LOW PRIORITY)**

1. **Documentation Updates**:
   - Update data structure docs with new schema
   - Create migration guide for users
   - Document 39-column CSV schema

2. **Git Commit** (when ready):
   ```
   Implement Refactor 005: Complete metadata integration pipeline
   
   ‚úÖ Phase 1: Enhanced schema with CSV integration (data_organizer.py)
   ‚úÖ Phase 2: Pipeline scripts updated for dictionary format
   ‚úÖ Phase 3: 39-column CSV export with raw metadata
   
   üéØ Result: Complete metadata lineage legacy CSV ‚Üí SAM2 ‚Üí build scripts
   ```

---

## **‚ö†Ô∏è CRITICAL NOTES FOR NEXT AGENT**

### **Working Implementation Location**:
- **File**: `segmentation_sandbox/scripts/data_organization/data_organizer_refactor_test.py`
- **Status**: Functional but contains debug code that needs cleanup
- **Next Step**: Clean up and migrate to production `data_organizer.py`

### **Pipeline Compatibility**:
- **Current State**: Enhanced metadata generation works
- **Blocker**: Pipeline scripts not updated to handle dictionary format
- **Risk**: Using enhanced metadata will break existing pipeline until scripts updated

### **Files Still Needing Updates**:
1. `experiment_metadata.py` - Core metadata utilities
2. `export_sam2_metadata_to_csv.py` - CSV export enhancement  
3. `gdino_detection.py` - Detection script compatibility
4. `sam2_segmentation.py` - Segmentation script compatibility

---

## **üèÜ ACCURATE FINAL STATUS**

**Refactor 005 (Fundamental Metadata Integration): üü° PARTIALLY COMPLETE**

**Current Achievement**:
- ‚úÖ Enhanced schema design and implementation (Phase 1)
- ‚úÖ CSV metadata integration working
- ‚úÖ JSON serialization fixes implemented
- ‚úÖ Magic number elimination confirmed (already done)

**Still Required**:
- ‚ùå Pipeline script updates (Phase 2)
- ‚ùå Enhanced CSV export (39 columns)
- ‚ùå Production migration from test files
- ‚ùå Backwards compatibility implementation

**Ready for**: Phase 2 implementation by next agent with clear working foundation.

---

## üîé Repo Reality Check & Actions (added by Codex)

What‚Äôs implemented in repo now:
- Enhanced schema is live in `segmentation_sandbox/scripts/data_organization/data_organizer.py` (list ‚Üí dict `image_ids`, `raw_image_data_info`, `processed_image_size_px`).
- CSV exporter `segmentation_sandbox/scripts/utils/export_sam2_metadata_to_csv.py` flattens to a 39‚Äëcolumn CSV, including raw and well metadata.
- Build03A computes `px_dim_raw = Height (um) / Height (px)` directly; no magic number remains.
- `ExperimentMetadata` supports both list and dict `image_ids` for compatibility.

Gaps found:
- Channel key mismatch: organizer writes `'Channel'` while exporter expects `'BF Channel'` ‚Üí nulls in output.
- Organizer doesn‚Äôt populate `microscope`/`nd2_series_num`; exporter has columns.
- Hardcoded absolute NFS paths in organizer reduce portability.
- `scripts/utils/experiment_metadata_utils.py` missing; referenced by SAM2/grounded utils, plus `get_video_info(...)` usage.
- Some places iterate dict `image_ids` without sorting keys.

Remediation plan applied in this change set:
- Align keys and add fallbacks so exporter accepts `'Channel'` when `'BF Channel'` absent.
- Populate optional `microscope`/`nd2_series_num` when present in legacy CSV.
- Parameterize CSV path resolution: prefer repo‚Äërelative `metadata/built_metadata_files/{experiment}_metadata.csv`, fall back to `MORPHSEQ_METADATA_ROOT` env var, then legacy absolute path.
- Add minimal `experiment_metadata_utils.py` shim with `load_experiment_metadata`, `get_image_id_paths`, and `get_video_info` to unblock SAM2 utils.
- Ensure exporter iterates `sorted(image_ids.keys())` for deterministic temporal order.
- Add dummy test scripts to exercise the bridge and verify required Build03A columns exist.

### ‚úÖ Codex Changes Applied (2025‚Äë08‚Äë29)

- `segmentation_sandbox/scripts/data_organization/data_organizer.py`
  - CSV path resolution made portable with repo‚Äërelative and `MORPHSEQ_METADATA_ROOT` fallbacks.
  - Image metadata: write `'BF Channel'` (and alias `'bf_channel'`), include `microscope`, `nd2_series_num` if present.
  - `source_well_metadata_csv` set to repo‚Äërelative hint.
- `segmentation_sandbox/scripts/utils/export_sam2_metadata_to_csv.py`
  - Deterministic iteration over `image_ids` dict via sorted keys.
  - Fallback handling for `'Channel'` ‚Üí `'BF Channel'` in raw metadata.
  - CSV schema validated at 39 columns.
- `segmentation_sandbox/scripts/utils/experiment_metadata_utils.py` (NEW)
  - Shim providing `load_experiment_metadata`, `get_image_id_paths`, `get_video_info` used by SAM2/GDINO utils.
- Dummy runners (NEW):
  - `segmentation_sandbox/scripts/tests/dummy_make_metadata_and_export.py` ‚Üí fabricates enhanced metadata and exports CSV.
  - `segmentation_sandbox/scripts/tests/dummy_verify_build03A_columns.py` ‚Üí verifies Build03A‚Äërequired columns exist.

### üß™ Local Validation Performed

- Created venv, installed `pandas`, ran dummy export and verification.
- Exporter produced CSV with 39 columns; Build03A‚Äërequired columns present.

### üîú Next Steps for the Next Agent

- Run exporter on real SAM2 JSON and compare non‚Äënull rates for all 39 columns; confirm `BF Channel` and time fields are populated across datasets.
- Review and, if needed, refactor `segmentation_sandbox/scripts/utils/sam2_utils.py` and `.../grounded_sam_utils.py` to consistently use the new shim and ensure sorted `image_ids` usage everywhere.
- Confirm that all downstream pipeline scripts (03_gdino_detection.py, 04_sam2_segmentation.py, 05_sam2_qc_analysis.py, 06_export_masks.py) work when `image_ids` is a dict; update any remaining list‚Äëassumptions.
- Document the finalized 39‚Äëcolumn CSV schema in repo docs and align any notebook/users with the new fields.
- Optionally add a small config (YAML) to specify metadata roots and masks paths instead of relying on environment defaults.
